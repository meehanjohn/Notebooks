{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Exploring a Soccer Match Database\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "I have chosen to analyze the [European Soccer Database](https://www.kaggle.com/hugomathien/soccer). The database includes approximately 26,000 records of soccer matches, including team layout and game outcome. There is also data regarding player and team attributes (taken from the FIFA video games), and values for betting odds on each game across several online gambling platforms.\n",
    "\n",
    "My focus will be on the players making up each team, team formations, and their respective impacts on team performance. Through this project, I will attempt to draw conclusions about factors influencing team performance based on the available data.\n",
    "\n",
    "I chose this database and to focus on performance because of the connection with my work background. I am interested in improving the performance of a manufacturing organization which produces only custom made-to-order products. I would like to draw a parallel between analyzing team sport performance and team business performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing\n",
    "\n",
    "First I import the packages relevant for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3                      # to read the raw database file in .sqlite format\n",
    "import pandas as pd                 # for creating and modifying dataframes\n",
    "import matplotlib.pyplot as plt     # for data visualization\n",
    "import seaborn as sb                # to clean up visualizations\n",
    "import os                           # to locate files within the directory\n",
    "import numpy as np                  #\n",
    "\n",
    "# visualizations will render in-browser\n",
    "%matplotlib inline                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling'></a>\n",
    "## Data Wrangling\n",
    "\n",
    "The file comes in .sqlite format, so it must be unpacked and imported into dataframes to be manipulated. Below, I will check out what data is included with each table and begin to shape the data so that it is useful for my analysis.\n",
    "\n",
    "### General Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()                     # ensures that the full path is being used\n",
    "database = path + '\\\\database.sqlite'  # even though the file should be in the same folder\n",
    "\n",
    "con = sqlite3.connect(database)        # establish a connection with the database\n",
    "tables = pd.read_sql(                  # write a query to see all tables\n",
    "    \"\"\"\n",
    "    SELECT * FROM sqlite_master\n",
    "    WHERE type='table';\n",
    "    \"\"\",con=con)\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables\n",
    "I know from reading the documentation provided with the database that the 2 `_Attributes` tables are based on data from the FIFA video games. I will not need them to support my analysis, so they will not be brought into dataframes.\n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write query to import player table\n",
    "player_df = pd.read_sql( \n",
    "    \"\"\"\n",
    "    SELECT player_api_id as id, player_name, birthday, height, weight \n",
    "    FROM Player;\n",
    "    \"\"\", con=con, index_col='id', parse_dates=['birthday'])\n",
    "\n",
    "# observe column names and datatypes\n",
    "player_df.info()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query to import league table\n",
    "league_df = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM League;\n",
    "    \"\"\", con=con, index_col='id')\n",
    "\n",
    "# observe column names and datatypes\n",
    "league_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_df = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM team;\n",
    "    \"\"\", con=con, index_col='team_api_id')\n",
    "team_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `Match` table contains the majority of the data pertinent to my analysis, I will try to `JOIN` the other tables as early as possible so that I can drop everything that I don't need and avoid doing any more operations.\n",
    "\n",
    "First I pull the `Country` and `League` names directly in, using an `INNER JOIN` on their respective ID's.\n",
    "\n",
    "I also have the `%%time` magic called because this table is so large; I want to also keep track of how long operations take and attempt to remove unnecessary steps at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT Country.name as country, League.name as league, match.*\n",
    "    FROM match\n",
    "    JOIN Country ON Country.id = match.country_id\n",
    "    JOIN League ON League.id = match.league_id;\n",
    "    \"\"\", con=con, index_col='id', parse_dates=['date'])\n",
    "match_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df_copy = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM match;\n",
    "    \"\"\", con=con, index_col='id', parse_dates=['date'])\n",
    "match_df_copy = match_df_copy[match_df_copy.columns[:76]]\n",
    "match_df_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first issues I will need to tackle is reducing the amount of columns in this table. The above output shows that there are 116 columns, too many to be listed using `.info()`. I will need to print a list of the columns to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, column in enumerate(list(match_df)):\n",
    "    print('%i: %s' % (num,column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like I will want to lose all of the columns after 77, because they contain metrics that I am not interested in for this analysis. In this case, I find it much easier to lose the columns within Pandas rather than SQL, because I want to use a numbered range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns by index\n",
    "match_df = match_df[match_df.columns[:78]]\n",
    "match_df_copy = match_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will convert the other column values from ID numbers to their respective string values. I still need to do this for the home and away team names, but it was already tackled in the SQL `JOIN` earlier for the 'Country' and 'League' columns. All I need to do is drop those ID columns as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_dict = team_df['team_long_name'].to_dict()\n",
    "match_df['home_team_api_id'] = match_df['home_team_api_id'].astype('int')\n",
    "match_df['home_team_name'] = match_df['home_team_api_id'].replace(values_dict)\n",
    "match_df['away_team_api_id'] = match_df['away_team_api_id'].astype('int')\n",
    "match_df['away_team_name'] = match_df['away_team_api_id'].replace(values_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns by name\n",
    "match_df.drop(['country_id', 'league_id', 'home_team_api_id', 'away_team_api_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the remaining columns, I can see that many of the rows contain null values.\n",
    "\n",
    "Below I will drop any rows containing null values using the `dropna` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_df.dropna(how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that each of the columns contains the same number of non-null entries, so there are no empty fields. There are still over 21,000 records even after being selective with the data, which should be plenty for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have gotten from the original 116 columns down to 76, which is still not quite easily readable. This is partially because both teams have 11 players, each of which has 3 dedicated columns in this table:\n",
    "- `home/away_player_N` - the API ID of the player in that position\n",
    "- `home/away_player_XN`- the 'X' coordinate position of the player on the field\n",
    "- `home/away_player_YN`- the 'Y' coordinate position of the player on the field\n",
    "\n",
    "To me, it makes more sense to condense these 3 fields for each player into a dictionary which specifices the `X,Y` coordinate set of the player's location:\n",
    "```\n",
    "{player: (x_coord, y_coord)}\n",
    "```\n",
    "\n",
    "To begin, I make a list of all of the player-related columns (they all have the word 'player' in the name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list comprehension of all the column names with the word 'player'\n",
    "player_cols = [col for col in match_df.columns if 'player' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = {}\n",
    "\n",
    "for i in range(1,12):\n",
    "    home_str = 'home_player_'\n",
    "    away_str = 'away_player_'\n",
    "    players[home_str+str(i)] = (home_str+'X'+str(i), home_str+'Y'+str(i))\n",
    "    players[away_str+str(i)] = (away_str+'X'+str(i), away_str+'Y'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_dict = player_df['player_name'].to_dict()\n",
    "\n",
    "def posit_dict(x, y, z):\n",
    "    return dict([(x,(int(y),int(z)))])\n",
    "\n",
    "for player in players.keys():\n",
    "    df_name = player + '_coords'\n",
    "    player_x = players[player][0]\n",
    "    player_y = players[player][1]\n",
    "    match_df[player] = match_df[player].astype('int')\n",
    "    match_df[player] = match_df[player].replace(values_dict)\n",
    "    match_df[df_name] = match_df.apply(lambda x: posit_dict(x[player], x[player_x], x[player_y]), axis=1)\n",
    "    match_df.drop([player, player_x, player_y], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, column in enumerate(list(match_df)):\n",
    "    print('%i: %s' % (num,column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it seems that we have a manageable number of columns, and we can be confident that each row contains all of the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "> **Tip**: Now that you've trimmed and cleaned your data, you're ready to move on to exploration. Compute statistics and create visualizations with the goal of addressing the research questions that you posed in the Introduction section. It is recommended that you be systematic with your approach. Look at one variable at a time, and then follow it up by looking at relationships between variables.\n",
    "\n",
    "### Research Question 1: Which team formations are the most effective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of popular team formations\n",
    "form_ids = range(8)\n",
    "form_names = ['4-4-2','4-3-3','3-4-3','3-6-1','3-4-2-1','4-5-1','5-2-2-1','3-5-2']\n",
    "\n",
    "\n",
    "form_arrays = {'4-4-2':[[1,1],[2,3],[4,3],[6,3],[8,3],[2,7],[4,7],[6,7],[8,7],[4,10],[6,10]],\n",
    "               '4-3-3':[[1,1],[2,3],[4,3],[6,3],[8,3],[3,7],[5,7],[7,7],[3,10],[5,10],[7,10]],\n",
    "               '3-4-3':[[1,1],[3,3],[5,3],[7,3],[2,7],[4,7],[6,7],[8,7],[3,10],[5,10],[7,10]],\n",
    "               '3-6-1':[[1,1],[3,3],[5,3],[7,3],[1,7],[3,7],[4,7],[6,7],[7,7],[9,7],[5,9]],\n",
    "               '3-4-2-1':[[1,1],[3,3],[5,3],[7,3],[2,7],[4,7],[6,7],[8,7],[4,9],[6,9],[5,10]],\n",
    "               '4-5-1':[[1,1],[2,3],[4,3],[6,3],[8,3],[1,7],[3,7],[5,7],[7,7],[9,7],[5,10]],\n",
    "               '5-2-2-1':[[1,1],[1,3],[3,3],[5,3],[7,3],[9,3],[4,5],[6,5],[4,7],[6,7],[5,11]],\n",
    "               '3-5-2':[[1,1],[3,3],[5,3],[7,3],[1,7],[3,7],[5,7],[7,7],[9,7],[4,10],[6,10]]}\n",
    "               \n",
    "pop_form_df = pd.DataFrame(data=form_arrays)\n",
    "pop_form_df\n",
    "\n",
    "#pop_form_df = pd.DataFrame(np.swapaxes(np.array(form_arrays),2,2), columns=form_names, index='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for the difference between home & away goals scored\n",
    "match_df['Score_Delt'] = match_df['home_team_goal']-match_df['away_team_goal']\n",
    "\n",
    "# Add another column specifying if the home or away team won\n",
    "bins = [np.NINF, -0.1, 0.1, np.Inf]\n",
    "match_df['Outcome'] = pd.cut(match_df['Score_Delt'], bins, labels=['away','tie','home'], include_lowest=True)\n",
    "\n",
    "# Verify that the output makes sense\n",
    "match_df[['home_team_goal','away_team_goal','Score_Delt','Outcome']].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_coords_list = list(match_df.filter(regex='home.*_coords', axis=1))\n",
    "away_coords_list = list(match_df.filter(regex='away.*_coords', axis=1))\n",
    "home_temp_dict = match_df[home_coords_list].apply(lambda x: x.tolist(), axis=1)\n",
    "away_temp_dict = match_df[away_coords_list].apply(lambda x: x.tolist(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_formations = []\n",
    "away_formations = []\n",
    "\n",
    "for row in home_temp_dict:\n",
    "    home_team_formation = []\n",
    "    for dict in row:\n",
    "        home_team_formation.append(list(dict.values())[0])\n",
    "    home_formations.append(home_team_formation)\n",
    "    \n",
    "for row in away_temp_dict:\n",
    "    away_team_formation = []\n",
    "    for dict in row:\n",
    "        away_team_formation.append(list(dict.values())[0])\n",
    "    away_formations.append(away_team_formation)\n",
    "\n",
    "home_formations = np.array(home_formations)\n",
    "away_formations = np.array(away_formations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(home_formations.shape)\n",
    "print(away_formations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "test_home = home_formations.reshape((234971,2))\n",
    "x = test_home[:,0]\n",
    "y = test_home[:,1]\n",
    "\n",
    "p_h = plt.hist2d(x,y,bins=11,norm=LogNorm())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_away = away_formations.reshape((234971,2))\n",
    "x = test_away[:,0]\n",
    "y = test_away[:,1]\n",
    "\n",
    "p_a = plt.hist2d(x,y,bins=11,norm=LogNorm())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 2: Which players affect team performance the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df_copy.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue to explore the data to address your additional research\n",
    "#   questions. Add more headers as needed if you have more questions to\n",
    "#   investigate.\n",
    "cols = ['home_team_api_id','away_team_api_id','home_team_goal','away_team_goal']\n",
    "pd.plotting.scatter_matrix(match_df_copy[cols], figsize=(15,15));\n",
    "\n",
    "# plot the heatmap\n",
    "#sb.heatmap(corr, \n",
    "#        xticklabels=corr.columns,\n",
    "#        yticklabels=corr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Conclusions\n",
    "\n",
    "> **Tip**: Finally, summarize your findings and the results that have been performed. Make sure that you are clear with regards to the limitations of your exploration. If you haven't done any statistical tests, do not imply any statistical conclusions. And make sure you avoid implying causation from correlation!\n",
    "\n",
    "> **Tip**: Once you are satisfied with your work here, check over your report to make sure that it is satisfies all the areas of the rubric (found on the project submission page at the end of the lesson). You should also probably remove all of the \"Tips\" like this one so that the presentation is as polished as possible.\n",
    "\n",
    "## Submitting your Project \n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from subprocess import call\n",
    "#call(['python', '-m', 'nbconvert', 'Investigate_a_Dataset.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
